{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "import sklearn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available is cuda\n",
      "Sample Rate of model: 16000\n",
      "Dimension of x_f is : (7744, 1, 149, 768)\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# First things first! Set a seed for reproducibility.\n",
    "# https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "##################################################################################################\n",
    "writer = SummaryWriter()\n",
    "writer = SummaryWriter(\"wav2vec_base_model\")\n",
    "writer = SummaryWriter(comment=\"2D conv layer architecture from edge impulse ; lr = 0.0001; 100 epochs; 100 hours data\")\n",
    "##################################################################################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device available is', device)\n",
    "# wav2vec2.0\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "print(\"Sample Rate of model:\", bundle.sample_rate)\n",
    "\n",
    "model_wav2vec = bundle.get_model().to(device)\n",
    "## Convert audio to numpy to wav2vec feature encodings\n",
    "def conv_audio_data (filename) :\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    waveform = waveform.to(device)\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        print('Mismatched sample rate')\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    emission, _ = model_wav2vec(waveform)\n",
    "    emission = emission.cpu().detach().numpy()\n",
    "    return emission\n",
    "\n",
    "x_f = []\n",
    "y_f = []\n",
    "x_s = []\n",
    "y_s = []\n",
    "# get all stutter data\n",
    "path_stutter  = \"Dataset/all_stutter/\"\n",
    "files_stutter = os.listdir(path_stutter)\n",
    "\n",
    "for filename in glob.glob(os.path.join(path_stutter, '*.wav')):\n",
    "    stutter_np = conv_audio_data(filename)\n",
    "    x_s.append(stutter_np)\n",
    "    y_s.append(1)\n",
    "\n",
    "# get all fluent data\n",
    "discarded = 0\n",
    "#FIXME :: How can I avoid discarding the mismatched samples?\n",
    "path_fluent  = \"Dataset/all_fluent/\"\n",
    "files_fluent = os.listdir(path_fluent)\n",
    "for filename in glob.glob(os.path.join(path_fluent, '*.wav')):\n",
    "    fluent_np = conv_audio_data(filename)\n",
    "    # fluent_np --> (1, 149, 768)\n",
    "    if ((np.shape(fluent_np)[0] != 1) |(np.shape(fluent_np)[1] != 149) | (np.shape(fluent_np)[2] != 768)) :\n",
    "        discarded += 1\n",
    "    else:\n",
    "        x_f.append(fluent_np)\n",
    "        y_f.append(0)\n",
    "\n",
    "# print('Before normalize', x_f)\n",
    "print('Dimension of x_f is :', np.shape(x_f))\n",
    "# Shuffle all data within a class so that we have samples from all podcasts.\n",
    "random.shuffle(x_f)\n",
    "random.shuffle(x_s)\n",
    "\n",
    "# 100 samples each for 100  mins training\n",
    "x_f_train = x_f[0:2000]\n",
    "y_f_train = y_f[0:2000]\n",
    "x_s_train = x_s[0:2000]\n",
    "y_s_train = y_s[0:2000]\n",
    "\n",
    "# validation dataset\n",
    "# x_f_valid = x_f[2001:2221]\n",
    "# y_f_valid = y_f[2001:2221]\n",
    "# x_s_valid = x_s[2001:2221]\n",
    "# y_s_valid = y_s[2001:2221]\n",
    "\n",
    "# 100 samples each for 10 mins testing\n",
    "x_f_test = x_f[-100:-1]\n",
    "y_f_test = y_f[-100:-1]\n",
    "x_s_test = x_s[-100:-1]\n",
    "y_s_test = y_s[-100:-1]\n",
    "\n",
    "# FIXME :: Shuffle this later on so that all classesa re not given sequentially for training\n",
    "x_train = x_f_train + x_s_train\n",
    "y_train = y_f_train + y_s_train\n",
    "\n",
    "x_test = x_f_test + x_s_test\n",
    "y_test = y_f_test + y_s_test\n",
    "\n",
    "# x_valid = x_f_valid + x_s_valid\n",
    "# y_valid = y_f_valid + y_s_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples to train =  4000\n",
      "Number of samples to test =  198\n"
     ]
    }
   ],
   "source": [
    "## Hyper parameters\n",
    "batch_size = 512\n",
    "num_epochs = 400\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# split data and translate to dataloader\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)\n",
    "\n",
    "n_samples_train = np.shape(x_train)[0]\n",
    "n_samples_test = np.shape(x_test)[0]\n",
    "# n_samples_valid = np.shape(x_valid)[0]\n",
    "print('Number of samples to train = ', n_samples_train)\n",
    "print('Number of samples to test = ', n_samples_test)\n",
    "# print('Number of samples for validation = ', n_samples_valid)\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset) :\n",
    "    def __init__(self,x,y, n_samples) :\n",
    "        # data loading\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index) :\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self) :    \n",
    "        return self.n_samples      \n",
    "\n",
    "train_dataset = AudioDataset(x_train,y_train,n_samples_train)\n",
    "test_dataset = AudioDataset(x_test,y_test,n_samples_test)\n",
    "# valid_dataset = AudioDataset(x_valid,y_valid,n_samples_valid)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "# valid_loader = DataLoader(dataset=valid_dataset,\n",
    "#                           batch_size=batch_size,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 23.69 GiB total capacity; 17.23 GiB already allocated; 1.58 GiB free; 19.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=60'>61</a>\u001b[0m         \u001b[39m#print('After final ',np.shape(out))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=61'>62</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=62'>63</a>\u001b[0m         \u001b[39m# log_probs = torch.nn.functional.log_softmax(out, dim=1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=64'>65</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=66'>67</a>\u001b[0m model \u001b[39m=\u001b[39m StutterNet(batch_size)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bideas2.research.northwestern.edu/home/past/payal/SSL-based-Automatic-Stutter-Detection-/base_wav2vec_100_mins_data_A.ipynb#ch0000003vscode-remote?line=68'>69</a>\u001b[0m \u001b[39m# Loss and optimizer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=906'>907</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/payal/miniconda3/envs/wav2vec/lib/python3.9/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.70 GiB (GPU 0; 23.69 GiB total capacity; 17.23 GiB already allocated; 1.58 GiB free; 19.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "\n",
    "class StutterNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(StutterNet, self).__init__()\n",
    "        # input shape = (batch_size, 1, 149,768)\n",
    "        # in_channels is batch size\n",
    "        self.layer1 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.layer1_bn = nn.BatchNorm2d(8)\n",
    "        # input size = (batch_size, 8, 74, 384)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=1, stride=2),\n",
    "            torch.nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.layer2_bn = nn.BatchNorm2d(16)\n",
    "        # input size = (batch_size, 16, 37, 192)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16* 37* 192,4000, bias=True)\n",
    "        # self.fc1_bn = nn.BatchNorm1d(4000)\n",
    "        # self.fc2 = nn.Linear(4000,100, bias=True)\n",
    "        # self.fc2_bn = nn.BatchNorm1d(500)\n",
    "        # self.fc3 = nn.Linear(500,100, bias=True)\n",
    "        # self.fc3_bn = nn.BatchNorm1d(100)\n",
    "        # self.fc4 = nn.Linear(100,10, bias=True)\n",
    "        # self.fc4_bn = nn.BatchNorm1d(10)\n",
    "        self.fc5 = nn.Linear(4000,2, bias=True)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('Before Layer1',np.shape(x))\n",
    "        out = self.layer1(x)\n",
    "        # print('After layer 1',np.shape(out))\n",
    "        out = self.layer2(out)\n",
    "        # print('After layer 2',np.shape(out))\n",
    "        out  = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.fc1_bn(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.fc2_bn(out)\n",
    "        # out = self.fc3(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.fc3_bn(out)\n",
    "        # out = self.fc4(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.fc4_bn(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.sm(out)\n",
    "        #print('After final ',np.shape(out))\n",
    "\n",
    "        # log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = StutterNet(batch_size).to(device)\n",
    "print(model)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#weighted loss\n",
    "#criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1, 3]).to(device)) # Class 0 is 75% of the total dataset \n",
    "#criterion = nn.LogSoftmax()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "loss_hist = []\n",
    "epoch_hist = []\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    n_correct = 0\n",
    "    for i, (features, labels) in enumerate(train_loader):  \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.reshape(labels,(np.shape(labels)[0],))\n",
    "        labels = labels.to(torch.int64)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.cpu().detach().numpy())\n",
    "        epoch_hist.append(epoch*np.ceil(n_samples_train/batch_size)+i)\n",
    "\n",
    "        # Compute Training Accuracy\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct = (labels == predicted_labels).sum()\n",
    "        acc = 100.0 * n_correct / outputs.shape[0]\n",
    "        # visualisation\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)  \n",
    "        writer.add_scalar(\"Accuracy/train\", acc, epoch)  \n",
    "        \n",
    "        if (i+1) % 2 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}, Acc : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, '/home/past/payal/SSL-based-Automatic-Stutter-Detection-/500_epoch_100_min_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4a211d82b0>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_hist, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StutterNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (layer1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (layer2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=113664, out_features=4000, bias=True)\n",
       "  (fc2): Linear(in_features=4000, out_features=500, bias=True)\n",
       "  (fc3): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (fc5): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = torch.load('/home/past/payal/SSL-based-Automatic-Stutter-Detection-/500_epoch_100_min_2.pth')\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test dataset is : 58.58585858585859 %\n",
      "Precision of the network on test dataset is : 0.5977011494252874\n",
      "Recall of the network on test dataset is : 0.5252525252525253\n",
      "F1 Score of the network on test dataset is : 0.5591397849462367\n"
     ]
    }
   ],
   "source": [
    "# test_loader = DataLoader(dataset=test_dataset,\n",
    "#                           batch_size=batch_size,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=1)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "   n_correct = 0\n",
    "   # Compute F1 score, precision and recall\n",
    "   predicted_stutter = 0\n",
    "   labels_stutter = 0\n",
    "   correct_stutter = 0\n",
    "   i = 0\n",
    "   final_label = []\n",
    "   final_predicted = []\n",
    "   for features, labels in test_loader:\n",
    "      #  print(labels)\n",
    "      #  print(np.shape(features))\n",
    "       features = features.to(device)\n",
    "      #  print(labels)\n",
    "       labels = labels.to(device)\n",
    "       outputs = model_test(features)\n",
    "    #    print(np.shape(outputs))\n",
    "       # max returns (value ,index)\n",
    "       _, predicted = torch.max(outputs.data, 1)\n",
    "      #  print(predicted)\n",
    "    #    print(np.shape(predicted))\n",
    "      #  label = torch.transpose(labels, -1, 0)\n",
    "       predicted = torch.reshape(predicted,(outputs.shape[0],1))\n",
    "      #  print('====================================')\n",
    "      #  print(predicted)\n",
    "      #  i = i+1\n",
    "    #    print(n_correct)\n",
    "      #  final_label.append(label)\n",
    "      #  final_predicted.append(predicted)\n",
    "      #  for i in range (0, outputs.shape[0]) :\n",
    "      #          # F1 score for stutter\n",
    "      #       if (predicted[i] == 1) :\n",
    "      #           predicted_stutter +=1\n",
    "      #       if (labels[i] == 1) :\n",
    "      #           labels_stutter +=1   \n",
    "      #       if ((predicted[i] == 1) & (labels[i] == 1)):\n",
    "      #           correct_stutter +=1\n",
    "      #       if (predicted[i] == labels[i]) :\n",
    "      #           n_correct = n_correct + 1\n",
    "\n",
    "for i in range (0, len(predicted)) :\n",
    "    # F1 score for stutter\n",
    "    if (predicted[i] == 1) :\n",
    "        predicted_stutter +=1\n",
    "    if (labels[i] == 1) :\n",
    "        labels_stutter +=1   \n",
    "    if ((predicted[i] == 1) & (labels[i] == 1)):\n",
    "        correct_stutter +=1\n",
    "    if (predicted[i] == labels[i]) :\n",
    "                n_correct = n_correct + 1\n",
    "\n",
    "acc_test = 100*n_correct/n_samples_test\n",
    "print(f'Accuracy of the network on test dataset is : {acc_test} %')\n",
    "recall = correct_stutter/ labels_stutter\n",
    "precision = correct_stutter / predicted_stutter\n",
    "f1_score = 2 * precision * recall / (precision + recall)    \n",
    "print(f'Precision of the network on test dataset is : {precision}')\n",
    "print(f'Recall of the network on test dataset is : {recall}')\n",
    "print(f'F1 Score of the network on test dataset is : {f1_score}')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aede6801b29297ca4753447bcd9fb52ef6084259d335c2409ba40d2da343a20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
